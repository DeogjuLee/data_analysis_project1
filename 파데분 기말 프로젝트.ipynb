{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeogjuLee/data_analysis_project1/blob/main/%ED%8C%8C%EB%8D%B0%EB%B6%84%20%EA%B8%B0%EB%A7%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPkTIBCneChv",
        "outputId": "28cbab2c-fd28-44db-e313-06811a8ac438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (8,768 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "# í°íŠ¸ ì„¤ì¹˜\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ë¬¸ì œ í•´ê²°\n",
        "import matplotlib\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# ì„¸ì…˜ ì¬ì‹œì‘ í•  ê²ƒ!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6K5lXrtetbo",
        "outputId": "3e08c45a-74ff-410e-c5f8-623ea46dfb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "file_path =  \"/content/gdrive/My Drive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/\" #ê°ì ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# youtube apië¥¼ ìœ„í•´ ì„¤ì¹˜\n",
        "! pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWPXmFzunduz",
        "outputId": "bd9f9182-b380-4873-a3d2-0b0baa56e2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.137.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x-pVptHahml"
      },
      "source": [
        "# 1ï¸âƒ£ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2y69-2GdRCc"
      },
      "source": [
        "\n",
        "#### êµ¬ê¸€ë“œë¼ì´ë¸Œ ê³µìœ  ë§í¬ ë˜ëŠ” ìºê¸€ ë§í¬ë¥¼ í†µí•´ íŒŒì¼ì„ ë‹¤ìš´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEzKilypdS9g"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --no-cache-dir gdown\n",
        "# !gdown --folder '1qPbpKiUMvqdFarg_l1kpnhXmsL-QPzY5?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46RopwUP6xSj"
      },
      "source": [
        "#### ë°ì´í„° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "J_y6bZ9ed4kH",
        "outputId": "7282cc56-d08f-4a41-a279-d45e37d24a65"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/BR_youtube_trending_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b1ade6a4fb77>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcountry_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/BR_youtube_trending_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# ë‚˜ë¼ ë¦¬ìŠ¤íŠ¸\n",
        "country_list = ['BR','CA','DE','FR','GB','IN','KR','MX','RU','US']\n",
        "# ê° ë‚˜ë¼ì˜ íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "file_paths = {}\n",
        "# country_listì— ìˆëŠ” ê° ë‚˜ë¼ì˜ íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
        "for country in country_list:\n",
        "#    file_paths[country] = f'/content/ìºê¸€ë°ì´í„°/{country}_youtube_trending_data.csv'\n",
        "    file_paths[country] = f'/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/{country}_youtube_trending_data.csv'\n",
        "\n",
        "\n",
        "# ê° ë‚˜ë¼ì˜ ë°ì´í„°í”„ë ˆì„ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "country_dfs = {}\n",
        "# country_listì— ìˆëŠ” ê° ë‚˜ë¼ì˜ íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "for country in country_list:\n",
        "    file_path = file_paths.get(country)\n",
        "    if file_path:\n",
        "        df = pd.read_csv(file_path)\n",
        "        country_dfs[country] = df\n",
        "    else:\n",
        "        print(f\"File path not found for {country}\")\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥ (ì˜ˆì‹œë¡œ í•œêµ­ì˜ ë°ì´í„°í”„ë ˆì„ ì¶œë ¥)\n",
        "country_dfs['KR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4xCzv1Rd4kK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# ê° ë‚˜ë¼ì˜ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
        "json_file_paths = {}\n",
        "\n",
        "# country_listì— ìˆëŠ” ê° ë‚˜ë¼ì˜ JSON íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
        "for country in country_list:\n",
        "    json_file_paths[country] = f'/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/{country}_category_id.json'\n",
        "\n",
        "country_id_title_dict = {}\n",
        "\n",
        "# JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ë°ì´í„° ì²˜ë¦¬\n",
        "for country, file_path in json_file_paths.items():\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "        country_json = json.load(json_file)\n",
        "\n",
        "    # 'id'ì™€ 'title' ì¶”ì¶œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥\n",
        "    country_id_title_dict[country] = {item['id']: item['snippet']['title'] for item in country_json['items']}\n",
        "\n",
        "for i in country_list:\n",
        "    print(i, len(country_id_title_dict[i])) #ë‚˜ë¼ë³„ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZqkR1ILd4kL"
      },
      "outputs": [],
      "source": [
        "country_id_title_dict['US'].keys() - country_id_title_dict['KR'].keys()\n",
        "# USì—ë§Œ ìˆëŠ”ê²ƒì€ 29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FEhk4ZSd4kM"
      },
      "outputs": [],
      "source": [
        "country_id_title_dict['US']['29']#29ì€ 'Nonprofits & Activism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaE38IdGd4kO"
      },
      "outputs": [],
      "source": [
        "# csv json ë³‘í•©\n",
        "for i in country_list:\n",
        "    # categoryIdë¥¼ ìˆ˜ì¹˜í˜•ì—ì„œ ë¬¸ìí˜•ìœ¼ë¡œ ë³€í™˜\n",
        "    country_dfs[i] = country_dfs[i].astype({'categoryId':'str'})\n",
        "    # 'id'ì™€ 'categoryId'ê°€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ 'category' ì—´ì„ ì¶”ê°€\n",
        "    country_dfs[i]['category'] = country_dfs[i]['categoryId'].map(country_id_title_dict[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVwGo-oxd4kS"
      },
      "outputs": [],
      "source": [
        "# # í•œêµ­ ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ë‚˜ë¼ë„ categoryê°€ ì—†ëŠ”ê²Œ ìˆìŒ\n",
        "# -> USë”•ì…”ë„ˆë¦¬ë¥¼ í•œêµ­ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ë‚˜ë¼ì—ê²Œ ì ìš© (ì–´ì°¨í”¼ 29 ì œì™¸ ë‚´ìš©ì€ ê°™ìŒ)\n",
        "for i in country_list:\n",
        "    country_dfs[i].loc[country_dfs[i].category.isnull()==1, 'category'] = 'Nonprofits & Activism'\n",
        "    print (\"\\nMissing values by columns:  \", i, country_dfs[i].isnull().sum())\n",
        "    country_dfs[i].columns\n",
        "\n",
        "#  description(ì„¤ëª…ë€)ì— ë„ê°’ì€ ìˆì§€ë§Œ, ë‹¤ë¥¸ ì¹¼ëŸ¼ì€ ë„ê°’ ì—†ìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khY76yk7d4kV"
      },
      "outputs": [],
      "source": [
        "# ìµœì¢… ë³‘í•©ëœ ê²°ê³¼ í™•ì¸ : ë‚˜ë¼ë³„(í–‰ê°œìˆ˜, ì¹¼ëŸ¼ê°œìˆ˜)\n",
        "for i in country_list:\n",
        "    print(i, country_dfs[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mum8ekKOahqi"
      },
      "source": [
        "# 2ï¸âƒ£ EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEPUBub7uCqr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "warnings.simplefilter(action='ignore',category = RuntimeWarning)\n",
        "pd.set_option('mode.chained_assignment',  None) #ëª¨ë“  ê²½ê³  ë„ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cixTjCvKpEoY"
      },
      "outputs": [],
      "source": [
        "def bar_for_cat(cat_list):\n",
        "    for i in cat_list:\n",
        "        new_df = kr_df.groupby(i).count().iloc[:,:1]\n",
        "        new_df_sorted = new_df.sort_values('video_id',ascending = False)\n",
        "        new_df_sorted_30 = new_df_sorted.head(20)\n",
        "        new_df_sorted_30[i] = new_df_sorted_30.index\n",
        "        new_df_sorted_30\n",
        "\n",
        "        plt.barh(new_df_sorted_30[i], new_df_sorted_30['video_id'])\n",
        "        plt.xlabel('counts')\n",
        "        plt.ylabel(i)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4loqaKTKp27-"
      },
      "outputs": [],
      "source": [
        "kr_df = country_dfs['KR']\n",
        "cat = ['channelTitle','category']\n",
        "bar_for_cat(cat)\n",
        "# ì±„ë„ëª… ê¸°ì¤€ìœ¼ë¡œ ì¸ê¸‰ë™ ì§„ì…íšŸìˆ˜ ìƒìœ„ 30ê°œ ì •ë ¬: íŒŒë¿Œë¦¬, ì´ëª‡ëª…, ìˆë°•ìŠ¤ ë“±ë“±ì˜ ìœ íŠœë²„\n",
        "# ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì¸ê¸‰ë™ ì§„ì…íšŸìˆ˜ ìƒìœ„ 30ê°œ ì •ë ¬ : Entertainmentê°€ ì••ë„ì  1ìœ„, People&Blog, Musicì´ ê·¸ ë’¤ë¥¼ ì´ìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk5VyzYVqEw7"
      },
      "outputs": [],
      "source": [
        "num = ['view_count','likes','dislikes','comment_count']\n",
        "num_kr_df = kr_df.loc[:,num]\n",
        "num_kr_df['likes_rate'] = num_kr_df['likes']/(num_kr_df['likes']+num_kr_df['dislikes'])\n",
        "num = ['view_count','likes','dislikes','comment_count','likes_rate']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(data = num_kr_df.corr(), annot=True, fmt = '.2f', linewidths=5, cmap='Reds')\n",
        "\n",
        "# ì¡°íšŒìˆ˜ë‘ ëŒ“ê¸€ìˆ˜ëŠ” ë†’ì€ ìƒê´€ê´€ê³„\n",
        "# dislikes ë‘ likesëŠ” ìŒì˜ ìƒê´€ê´€ê³„ê°€ ì•„ë‹˜. ì˜¤íˆë ¤ ë¹„ë¡€í•¨. (ì‹«ì–´ìš”ë„ ì–´ëŠì •ë„ ì¸ì§€ë„ê°€ ìˆì–´ì•¼ ë‹¬ë¦´ ìˆ˜ ìˆë“¯ 'ì¢‹ì•„ìš”ì™€ ì‹«ì–´ìš”'ëŠ” ì–‘ì˜ ê´€ê³„!)\n",
        "# ë°˜ë©´ dislikesì™€ likes_rate[ì‹«ì–´ìš”/(ì¢‹ì•„ìš”+ì‹«ì–´ìš”)]ëŠ” ìŒì˜ ìƒê´€ê´€ê³„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5PYFSXazSai"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.boxplot((num_kr_df['likes'], num_kr_df['dislikes']))\n",
        "ax.set_xticklabels(('likes','dislikes'))\n",
        "# boxplotì„ ê·¸ë¦¬ëŠ” ê²Œ ì˜ë¯¸ê°€ ì—†ì„ ì •ë„ë¡œ ì´ìƒì¹˜ê°€ ë§ìŒ\n",
        "# likes ê°€ dislikes ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jT_ZeVkdC3B"
      },
      "outputs": [],
      "source": [
        "# ì•„ì›ƒë¼ì´ì–´ ë³´ì´ì§€ ì•Šê²Œ\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot((num_kr_df['likes'], num_kr_df['dislikes']),showfliers = False)\n",
        "ax.set_xticklabels(('likes','dislikes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzwAoLYWa0xx"
      },
      "source": [
        "# 3ï¸âƒ£ ì‚¬ìš©ì ê¸°ëŠ¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stnRIhxBa5p8"
      },
      "source": [
        "## ğŸŒ ê¸°ëŠ¥1 : ì¹´í…Œê³ ë¦¬ì™€ ë¶„ì„ê¸°ê°„ì„ ì…ë ¥í•˜ë©´ ê¸°ê°„ë‚´ ìƒìœ„ìœ íŠœë²„ ë³´ì—¬ì£¼ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA3uDu0xG7Pu"
      },
      "outputs": [],
      "source": [
        "kr_df = []\n",
        "kr_df = country_dfs['KR']\n",
        "kr_df['trending_date'] = pd.to_datetime(kr_df['publishedAt'])\n",
        "kr_df['publishedAt'] = pd.to_datetime(kr_df['trending_date'])\n",
        "\n",
        "# trending_dateê³¼ publishedAtë¥¼ ë³´ë©´ UTC(í˜‘ì • ì„¸ê³„ì‹œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì„)\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# UTC ì‹œê°„ëŒ€ë¡œ í•´ì„í•˜ì—¬ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "kr_df['trending_date'] = pd.to_datetime(kr_df['trending_date'])\n",
        "kr_df['publishedAt'] = pd.to_datetime(kr_df['publishedAt'])\n",
        "\n",
        "# í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜\n",
        "korea_timezone = pytz.timezone('Asia/Seoul')\n",
        "kr_df['trending_date'] = kr_df['trending_date'].dt.tz_convert('UTC').dt.tz_convert(korea_timezone)\n",
        "kr_df['publishedAt'] = kr_df['publishedAt'].dt.tz_convert('UTC').dt.tz_convert(korea_timezone)\n",
        "\n",
        "kr_df['trending_date_ymd'] = kr_df['trending_date'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoN3n7fnKA_7"
      },
      "outputs": [],
      "source": [
        "# ì¹´í…Œê³ ë¦¬ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°€ì¥ ì¸ê¸‰ë™ì— ë§ì´ ì˜¬ëë˜ ìœ íŠœë²„ ì¶”ì²œ\n",
        "\n",
        "def most_frequent_channel_withdates(category, start_date, end_date):\n",
        "    import matplotlib.pyplot as plt\n",
        "    # ì…ë ¥ë°›ì€ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ í•„í„°ë§\n",
        "    category_df = kr_df[kr_df['categoryId'] == category]\n",
        "\n",
        "    # ë‚ ì§œ ë²”ìœ„ ì„¤ì • í›„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "    start_date = pd.to_datetime(start_date, format=\"%Y-%m-%d\").date()\n",
        "    end_date = pd.to_datetime(end_date, format=\"%Y-%m-%d\").date()\n",
        "\n",
        "    # ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ë¡œ ë°ì´í„°í”„ë ˆì„ ìŠ¬ë¼ì´ì‹±\n",
        "    sliced_df = category_df[(category_df['trending_date_ymd'] >= start_date) & (category_df['trending_date_ymd'] <= end_date)]\n",
        "\n",
        "    # ê° ì±„ë„ë³„ ë“±ì¥ íšŸìˆ˜ë¥¼ ì„¸ê¸°\n",
        "    channel_counts = sliced_df['channelTitle'].value_counts()\n",
        "\n",
        "    # ê° ì±„ë„ë³„ í‰ê·  ë“±ì¥ íšŸìˆ˜ ê³„ì‚°\n",
        "    mean_counts = channel_counts.groupby(channel_counts.index).mean()\n",
        "    mean_counts = pd.DataFrame(mean_counts)\n",
        "\n",
        "    # ê°€ì¥ ë§ì´ ë“±ì¥í•œ ìƒìœ„ 10ê°œ ì±„ë„ê³¼ ê° í‰ê·  ì¸ê¸‰ë™ ì§„ì…íšŸìˆ˜\n",
        "    top_10_channels = channel_counts.head(10)\n",
        "    mean_counts_top10 = mean_counts.loc[top_10_channels.index.tolist(),:]\n",
        "    # print(mean_counts_top10)\n",
        "\n",
        "    # ë„˜íŒŒì´ í™œìš©í•œ ì½”ë“œ\n",
        "    import numpy as np\n",
        "    # Convert to numpy array for further processing\n",
        "    channels = channel_counts.index.to_numpy()\n",
        "    counts = channel_counts.to_numpy()\n",
        "\n",
        "    # Calculate mean counts for each channel (if needed)\n",
        "    mean_counts = np.mean(counts)\n",
        "\n",
        "    # Get top 10 channels\n",
        "    top_10_indices = np.argsort(-counts)[:10]\n",
        "    top_10_channels = channels[top_10_indices]\n",
        "    top_10_counts = counts[top_10_indices]\n",
        "\n",
        "    # # Plot results\n",
        "    # plt.barh(top_10_channels, top_10_counts)\n",
        "    # plt.xlabel('í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì¸ê¸‰ë™ì— ì˜¤ë¥¸ íšŸìˆ˜')\n",
        "    # plt.show()\n",
        "\n",
        "    # if top_10_channels.size == 0:\n",
        "    #     print('ê¸°ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìœ íŠœë²„ê°€ ì¸ê¸‰ë™ì— ì˜¬ë¼ê°„ ê²½ìš°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nê·¸ë˜í”„ ì¶œë ¥ì´ ì—†ì„ ê²½ìš° ê¸°ê°„ì„ ë„‰ë„‰í•˜ê²Œ 3ë‹¬ ì´ìƒ ì¡ìœ¼ì‹œê¸¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤')\n",
        "\n",
        "    # ë‚˜ëˆ” í°íŠ¸ ì„¤ì •\n",
        "    plt.rc('font', family='NanumGothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "    plt.barh(mean_counts_top10.index, mean_counts_top10['count'])\n",
        "    plt.xlabel('í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì¸ê¸‰ë™ì— ì˜¤ë¥¸ íšŸìˆ˜')\n",
        "    plt.show()\n",
        "    print('ê¸°ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìœ íŠœë²„ê°€ ì¸ê¸‰ë™ì— ì˜¬ë¼ê°„ ê²½ìš°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\nê·¸ë˜í”„ ì¶œë ¥ì´ ì—†ì„ ê²½ìš° ê¸°ê°„ì„ ë„‰ë„‰í•˜ê²Œ 3ë‹¬ ì´ìƒ ì¡ìœ¼ì‹œê¸¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWcGkSvKk_gJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def validate_date(date_str):\n",
        "    # ë‚ ì§œ í˜•ì‹ì„ ê²€ì¦í•˜ëŠ” ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´\n",
        "    date_pattern = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
        "    if re.match(date_pattern, date_str):\n",
        "        try:\n",
        "            # ë‚ ì§œ í˜•ì‹ì´ ë§ë‹¤ë©´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œì¸ì§€ í™•ì¸\n",
        "            datetime.strptime(date_str, '%Y-%m-%d')\n",
        "            return True\n",
        "        except ValueError:\n",
        "            return False\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHC3KaNKuVo"
      },
      "outputs": [],
      "source": [
        "# ì‚¬ìš©ìë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ì…ë ¥ ë°›ê¸°\n",
        "display(country_id_title_dict['KR']) #ì¹´í…Œê³ ë¦¬ ëª…ì„¸í‘œ ì¶œë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NhhCLPLdsIL"
      },
      "outputs": [],
      "source": [
        "category_input = input(\"ë¶„ì„í•˜ê³  ì‹¶ì€ ì¹´í…Œê³ ë¦¬idë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”! í•œêµ­ì—ì„œëŠ” ìœ„ ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ì¹´í…Œê³ ë¦¬idë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤: \")\n",
        "while True:\n",
        "    start_date_input = input('ë¶„ì„ ì‹œì‘ë‚ ì§œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (í˜•ì‹:yyyy-mm-dd): ')\n",
        "    if validate_date(start_date_input):\n",
        "        break\n",
        "    else:\n",
        "        print('í˜•ì‹ì— ë§ì¶° ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.')\n",
        "while True:\n",
        "    end_date_input = input('ë¶„ì„ ì¢…ë£Œë‚ ì§œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (í˜•ì‹:yyyy-mm-dd): ')\n",
        "    if validate_date(end_date_input):\n",
        "        break\n",
        "    else:\n",
        "        print('í˜•ì‹ì— ë§ì¶° ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.')\n",
        "# í•¨ìˆ˜ í˜¸ì¶œ\n",
        "most_frequent_channel_withdates(category_input,start_date_input,end_date_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b8uGRrba5sO"
      },
      "source": [
        "##ğŸŒ ê¸°ëŠ¥2 : ìœ íŠœë²„ì˜ ì‹œê°„ì— ë”°ë¥¸ ì¡°íšŒìˆ˜ ì¶”ì„¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dyb_4TqncW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def lowercase(name):\n",
        "    return name.lower()\n",
        "\n",
        "# kr_dfëŠ” ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "# kr_df = pd.read_csv('your_data.csv')\n",
        "\n",
        "kr_df = []\n",
        "kr_df = country_dfs['KR']\n",
        "kr_df['Search_name'] = kr_df['channelTitle'].apply(lowercase)\n",
        "\n",
        "name = input('ê¶ê¸ˆí•œ ìœ íŠœë²„ë¥¼ ì…ë ¥í•˜ì„¸ìš”:')\n",
        "\n",
        "while not pd.Series(kr_df['Search_name'].str.contains(name, case=False)).any():\n",
        "    print(\"í•´ë‹¹ ë¬¸ìì—´ì„ í¬í•¨í•˜ëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    name = input(\"ë‹¤ì‹œ ê²€ìƒ‰í•  ë¬¸ìì—´ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "\n",
        "result_df = kr_df[kr_df['Search_name'].str.contains(name, case=False)]\n",
        "result_unique = result_df.drop_duplicates(subset=['Search_name'])\n",
        "select_list = result_unique['channelTitle'].tolist()\n",
        "\n",
        "print(select_list)\n",
        "name = input('ìœ„ ë¦¬ìŠ¤íŠ¸ ì¤‘ ì›í•˜ëŠ” ìœ íŠœë²„ ì±„ë„ëª…ì„ ê·¸ëŒ€ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”:')\n",
        "\n",
        "select_df = result_unique[result_unique['Search_name'] == name.lower()]\n",
        "cate_num = int(select_df['categoryId'].iloc[0])\n",
        "\n",
        "kr_df['categoryId'] = kr_df['categoryId'].astype(int)\n",
        "cate_df = kr_df[kr_df['categoryId'] == cate_num]\n",
        "\n",
        "youtubers_df = cate_df.groupby('channelTitle')['view_count'].sum().sort_values(ascending=False)\n",
        "a = youtubers_df.index.tolist()\n",
        "\n",
        "filtered_df = kr_df[kr_df['channelTitle'].isin(a)].copy()\n",
        "filtered_df['publishedAt'] = pd.to_datetime(filtered_df['publishedAt'])\n",
        "filtered_df['year'] = filtered_df['publishedAt'].dt.year\n",
        "filtered_df['month'] = filtered_df['publishedAt'].dt.month\n",
        "\n",
        "monthly_median_views = filtered_df.groupby(['year', 'month'])['view_count'].median().reset_index()\n",
        "monthly_avg_views = filtered_df.groupby(['year', 'month'])['view_count'].mean().reset_index()\n",
        "\n",
        "top_youtuber = youtubers_df.idxmax()\n",
        "top_filtered_df = kr_df[kr_df['channelTitle'] == top_youtuber].copy()\n",
        "top_filtered_df['publishedAt'] = pd.to_datetime(top_filtered_df['publishedAt'])\n",
        "top_filtered_df['year'] = top_filtered_df['publishedAt'].dt.year\n",
        "top_filtered_df['month'] = top_filtered_df['publishedAt'].dt.month\n",
        "\n",
        "top_avg = top_filtered_df.groupby(['year', 'month'])['view_count'].mean().reset_index()\n",
        "\n",
        "name_df = kr_df[kr_df['channelTitle'] == name].copy()\n",
        "name_df['publishedAt'] = pd.to_datetime(name_df['publishedAt'])\n",
        "name_df['year'] = name_df['publishedAt'].dt.year\n",
        "name_df['month'] = name_df['publishedAt'].dt.month\n",
        "name_avg_views = name_df.groupby(['year', 'month'])['view_count'].mean().reset_index()\n",
        "\n",
        "compare_df = name_avg_views\n",
        "for df, suffix in zip([monthly_avg_views, monthly_median_views, top_avg], ['_2', '_3', '_4']):\n",
        "    compare_df = pd.merge(compare_df, df, on=['year', 'month'], how='inner', suffixes=('', suffix))\n",
        "\n",
        "compare_df['date'] = compare_df['year'].astype(str) + '-' + compare_df['month'].astype(str)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(compare_df['date'], compare_df['view_count'], marker='o', color='g', label='ê²€ìƒ‰í•œ ìœ íŠœë²„')\n",
        "#plt.plot(compare_df['date'], compare_df['view_count_2'], marker='o', color='r', label='ê°™ì€ ì¹´í…Œê³ ë¦¬ ìœ íŠœë²„ ì¡°íšŒìˆ˜ì˜ í‰ê· ')\n",
        "plt.plot(compare_df['date'], compare_df['view_count_3'], marker='o', color='b', label='ê°™ì€ ì¹´í…Œê³ ë¦¬ ìœ íŠœë²„ ì¡°íšŒìˆ˜ì˜ ì¤‘ê°„ê°’')\n",
        "#plt.plot(compare_df['date'], compare_df['view_count_4'], marker='o', color='y', label='ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ ìµœìƒìœ„ ìœ íŠœë²„')\n",
        "\n",
        "plt.title('ê°™ì€ ì¹´í…Œê³ ë¦¬ ìœ íŠœë²„ vs ê²€ìƒ‰í•œ ìœ íŠœë²„', fontsize=16)\n",
        "plt.xlabel('ë‚ ì§œ', fontsize=12)\n",
        "plt.ylabel('ì¡°íšŒìˆ˜', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb9QokfPa5vA"
      },
      "source": [
        "##ğŸŒ ê¸°ëŠ¥3 : ìœ íŠœë²„ì˜ ì¡°íšŒìˆ˜ë³„ ì‹«ì–´ìš” ì‚°ì ë„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4T2kKwKpemM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "# íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
        "directory_path = '/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/'\n",
        "\n",
        "# CSV íŒŒì¼ ì—´ê¸°\n",
        "with open(directory_path + 'KR_youtube_trending_data.csv', 'r', encoding='utf-8') as f:\n",
        "    data = csv.reader(f, delimiter=',')\n",
        "    header = next(data)\n",
        "\n",
        "    view_count = []\n",
        "    dislike = []\n",
        "    total_view_count = 0\n",
        "    total_dislike = 0\n",
        "    channel_name = input(\"ì±„ë„ ì…ë ¥: \")  # ìœ íŠœë²„ ì…ë ¥\n",
        "\n",
        "    for row in data:\n",
        "        if row[10] != '0' :\n",
        "            total_view_count += int(row[8])\n",
        "            total_dislike += int(row[10])\n",
        "        if row[4] == channel_name:\n",
        "            if row[10] != '0' :\n",
        "                view_count.append(int(row[8]))  # ìœ íŠœë²„ì— ë”°ë¥¸ ì¡°íšŒìˆ˜\n",
        "                dislike.append(int(row[10]))  # ìœ íŠœë²„ì— ë”°ë¥¸ ì‹«ì–´ìš”\n",
        "\n",
        "    # numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "    view_count = np.array(view_count)\n",
        "    dislike = np.array(dislike)\n",
        "\n",
        "    # ì „ì²´ ì¡°íšŒìˆ˜ì™€ ì‹«ì–´ìš” ìˆ˜ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ì¼ì°¨ê·¸ë˜í”„ë¡œ í‘œí˜„\n",
        "    dislike_ratio = total_dislike / total_view_count\n",
        "\n",
        "    # x ê°’ ë²”ìœ„ ìƒì„±\n",
        "    max_view_count = view_count.max()\n",
        "    x_values = np.arange(max_view_count + 1)\n",
        "\n",
        "    # y ê°’ ê³„ì‚°\n",
        "    y_values = dislike_ratio * x_values\n",
        "\n",
        "    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰)\n",
        "    plt.style.use('ggplot')\n",
        "    plt.rc('font', family='NanumBarunGothic')\n",
        "    plt.plot(x_values, y_values, color='blue', label='ë‹¤ë¥¸ ìœ íŠœë²„ì˜ ì‹«ì–´ìš” ë¹„ìœ¨')\n",
        "\n",
        "    # ìœ íŠœë²„ì˜ ì¡°íšŒìˆ˜ì— ë”°ë¥¸ ì‹«ì–´ìš” ìˆ˜ ì‚°ì ë„ë¡œ í‘œí˜„\n",
        "    plt.scatter(view_count, dislike, color='red', label='ìœ íŠœë²„ì˜ ì¡°íšŒìˆ˜ì— ëŒ€í•œ ì‹«ì–´ìš”')\n",
        "\n",
        "    # ê·¸ë˜í”„ ì œëª© ì„¤ì •\n",
        "    plt.title(channel_name + ' ì¡°íšŒìˆ˜/ì‹«ì–´ìš” ë¹„ìœ¨')\n",
        "\n",
        "    # xì¶• ë ˆì´ë¸” ì„¤ì •\n",
        "    plt.xlabel('ì¡°íšŒìˆ˜')\n",
        "\n",
        "    # yì¶• ë ˆì´ë¸” ì„¤ì •\n",
        "    plt.ylabel('ì‹«ì–´ìš”')\n",
        "\n",
        "    # ë²”ë¡€ ì¶”ê°€\n",
        "    plt.legend()\n",
        "\n",
        "    # ê·¸ë˜í”„ í‘œì‹œ\n",
        "    plt.show()\n",
        "\n",
        "print('''( â‰§á—œâ‰¦ ) íŒŒë€ìƒ‰ì˜ ê·¸ë˜í”„ëŠ” ë‹¤ë¥¸ ìœ íŠœë²„ì˜ ì‹«ì–´ìš” ë¹„ìœ¨ë¡œ,\n",
        "ğŸ‘ì…ë ¥í•œ ìœ íŠœë²„ì˜ ì‚°ì ë„(ë¹¨ê°„ìƒ‰)ê°€ ê·¸ë˜í”„(íŒŒë€ìƒ‰)ë³´ë‹¤ ë‚®ë‹¤ë©´ ë…¼ë€ì´ ì—†ëŠ” ìœ íŠœë²„ğŸ™†â€â™€ï¸,\n",
        "ê·¸ë˜í”„(íŒŒë€ìƒ‰)ë³´ë‹¤ ë†’ë‹¤ë©´ ë…¼ë€ì´ ìˆëŠ” ìœ íŠœë²„ğŸ™…â€â™€ï¸ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASmETReyb31O"
      },
      "source": [
        "##ğŸŒ ê¸°ëŠ¥4 : ì¸ê¸‰ë™ ì˜¤ë¥¸ ë™ì˜ìƒìˆ˜ì™€ ê·¸ ë¹„ìœ¨ ë³´ì—¬ì£¼ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NujnXSc-jzK"
      },
      "outputs": [],
      "source": [
        "# # youtube apië¥¼ ìœ„í•´ ì„¤ì¹˜\n",
        "# ! pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VITPXKdL-U2m"
      },
      "outputs": [],
      "source": [
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# ë³¸ì¸ì˜ apií‚¤ë¥¼ ë„£ê¸°\n",
        "mykey = 'AIzaSyCOCVTY3_Q5iZfO4i2env2XhiT9dabvong' # í˜„ì¬ ì¡°ì›ì˜ api í‚¤ê°€ ë“¤ì–´ê°€ ìˆìŒ\n",
        "youtube = build('youtube','v3',developerKey=mykey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KMmtl8Cs3hK"
      },
      "outputs": [],
      "source": [
        "# 1ë…„ë™ì•ˆ ì˜¬ë¦° ë¹„ë””ì˜¤ ê°œìˆ˜ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
        "import os\n",
        "import googleapiclient.discovery\n",
        "from datetime import datetime, timedelta\n",
        "def get_videos_from_channel(channel_id):\n",
        "    # ì±„ë„ì˜ 'uploads' ì¬ìƒëª©ë¡ ID ê°€ì ¸ì˜¤ê¸°\n",
        "    request = youtube.channels().list(\n",
        "        part=\"contentDetails\",\n",
        "        id=channel_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    uploads_playlist_id = response[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
        "    # ëª¨ë“  ë¹„ë””ì˜¤ ê°€ì ¸ì˜¤ê¸°\n",
        "    videos = []\n",
        "    next_page_token = None\n",
        "    one_year_ago = datetime.now() - timedelta(days=365)\n",
        "    while True:\n",
        "        request = youtube.playlistItems().list(\n",
        "            part=\"snippet\",\n",
        "            playlistId=uploads_playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "        for item in response[\"items\"]:\n",
        "            video_published_at = item[\"snippet\"][\"publishedAt\"]\n",
        "            video_published_at = datetime.strptime(video_published_at, '%Y-%m-%dT%H:%M:%SZ')\n",
        "            if video_published_at > one_year_ago:\n",
        "                videos.append(item)\n",
        "        next_page_token = response.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "    return videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb9USh2tsxSC"
      },
      "outputs": [],
      "source": [
        "# 1ë…„ê°„ì˜ í•´ë‹¹ ìœ íŠœë²„ í†µê³„ ì„¤ëª…í•˜ëŠ” í•¨ìˆ˜\n",
        "def get_youtube_channel_stat_video(channel_title):\n",
        "    import pandas as pd; from datetime import datetime, timedelta; import pytz\n",
        "    pd.options.mode.chained_assignment = None\n",
        "\n",
        "    filtered_df = country_dfs['KR'][country_dfs['KR']['channelTitle'] == channel_title]\n",
        "    id1 = filtered_df.iloc[0]['channelId']\n",
        "\n",
        "    filtered_df['publishedAt'] = pd.to_datetime(filtered_df['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ', utc=True)\n",
        "    kst = pytz.timezone('Asia/Seoul')\n",
        "    filtered_df['publishedAt'] = filtered_df['publishedAt'].dt.tz_convert(kst)\n",
        "\n",
        "    current_date_kst = datetime.now(kst)\n",
        "    one_year_ago_kst = current_date_kst - timedelta(days=365)\n",
        "    recent_videos = filtered_df[filtered_df['publishedAt'] >= one_year_ago_kst]\n",
        "    trending_recent_videos = recent_videos['video_id'].nunique()\n",
        "    num_recent_videos = len(get_videos_from_channel(id1))\n",
        "\n",
        "    request = youtube.channels().list(\n",
        "        part='statistics',\n",
        "        id=id1\n",
        "    )\n",
        "    response = request.execute()\n",
        "    total_video_count = int(response['items'][0]['statistics']['videoCount'])\n",
        "    unique_video_ids = filtered_df['video_id'].nunique()\n",
        "    ratio1 = (unique_video_ids / total_video_count) * 100\n",
        "    rounded_ratio1 = round(ratio1, 2)\n",
        "    ratio2 = (trending_recent_videos / num_recent_videos) * 100\n",
        "    rounded_ratio2 = round(ratio2, 2)\n",
        "\n",
        "    # return unique_video_ids, total_video_count, f'{rounded_ratio}%', trending_recent_videos, num_recent_videos\n",
        "    report = f'''\n",
        "    ìœ íŠœë²„ '{channel_title}'ì€ ê³„ì • ê°œì„¤ì‹œì ë¶€í„° ì§€ê¸ˆê¹Œì§€ {total_video_count}ê°œì˜ ì˜ìƒì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤.\n",
        "    ê·¸ì¤‘ {unique_video_ids}ê°œê°€ ì¸ê¸°ê¸‰ìƒìŠ¹ ë™ì˜ìƒì— ì˜¬ëìœ¼ë©°, ê·¸ ë¹„ìœ¨ì€ ğŸš©\\033[1m{rounded_ratio1}%\\033[0mì…ë‹ˆë‹¤.\n",
        "    ìµœê·¼ 1ë…„ê°„ì€ {num_recent_videos}ê°œì˜ ì˜ìƒì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤.\n",
        "    ê·¸ì¤‘ {trending_recent_videos}ê°œê°€ ì¸ê¸°ê¸‰ìƒìŠ¹ ë™ì˜ìƒì— ì˜¬ëìœ¼ë©°, ê·¸ ë¹„ìœ¨ì€ ğŸš©\\033[1m{rounded_ratio2}%\\033[0mì…ë‹ˆë‹¤.\n",
        "    '''\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nqwXKdIs0ED"
      },
      "outputs": [],
      "source": [
        "name = input('ê²€ìƒ‰í•˜ê³ í”ˆ ìœ íŠœë²„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”! : ')\n",
        "get_youtube_channel_stat_video(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Zv75P1bGk6"
      },
      "source": [
        "##ğŸŒ ê¸°ëŠ¥5 : ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì˜ìƒ ë³´ì—¬ì£¼ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOsb7nilbGk7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import csv\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
        "directory_path = '/content/gdrive/MyDrive/2024-1/ì´ˆíŒŒë°/ì´ˆíŒŒë°íŒ€í”Œ/ìºê¸€ë°ì´í„°/'\n",
        "\n",
        "# CSV íŒŒì¼ ì—´ê¸°\n",
        "f = open(directory_path + 'KR_youtube_trending_data.csv', 'r', encoding='utf-8')\n",
        "data = csv.reader(f, delimiter=',')\n",
        "header = next(data)  # í—¤ë” ì½ê¸°\n",
        "\n",
        "def category_keyword_videos(category, word):\n",
        "    unique_rows = []\n",
        "    unique_rows1 = []\n",
        "\n",
        "\n",
        "    for row in data:\n",
        "        if category == row[5] :\n",
        "            if word in row[1] :                 # ë‹¨ì–´ê°€ row[1]ì•ˆì— ìˆë‹¤ë©´\n",
        "                if row[1] not in unique_rows1 : # ì œëª©ì´ unique_rowsì— ì—†ë‹¤ë©´ (ì¤‘ë³µì œê±°)\n",
        "                    unique_rows1.append(row[1])\n",
        "                    unique_rows.append(row)\n",
        "\n",
        "\n",
        "    for row in unique_rows :\n",
        "        row[8] = int(row[8]) # ì¡°íšŒìˆ˜ ì •ìˆ˜ê°’ìœ¼ë¡œ ë³€ê²½\n",
        "\n",
        "\n",
        "    unique_rows.sort(key=lambda x: x[8], reverse=True)     # ì¡°íšŒìˆ˜ë¡œ ì •ë ¬\n",
        "\n",
        "\n",
        "    # row[1], row[2], row[3] ë°ì´í„°ë¥¼ ì¶”ì¶œ\n",
        "    filtered_rows = [[row[4], row[1], row[8]] for row in unique_rows]\n",
        "\n",
        "\n",
        "    # ë°ì´í„° í”„ë ˆì„ ìƒì„±\n",
        "    df = pd.DataFrame(filtered_rows, columns=['chennel', 'title', 'view_counts'])\n",
        "\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-ykgwrwbGk7"
      },
      "outputs": [],
      "source": [
        "# ì‚¬ìš©ìë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ ì…ë ¥ ë°›ê¸°\n",
        "display(country_id_title_dict['KR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T26cds_fT7b"
      },
      "outputs": [],
      "source": [
        "category = input('ì•Œê³ ì‹¶ì€ ì£¼ì œì˜ ì¹´í…Œê³ ë¦¬ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì‹œì˜¤: ')\n",
        "word = input('ì•Œê³  ì‹¶ì€ ì£¼ì œ(ë‹¨ì–´)ë¥¼ ì…ë ¥í•˜ì‹œì˜¤: ')\n",
        "category_keyword_videos(category, word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpOYBmSSb33J"
      },
      "source": [
        "##ğŸŒ ê¸°ëŠ¥6 : ë‚˜ë¼ë³„ë¡œ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ê°€ ì¸ê¸° ë§ì€ì§€ ë³´ì—¬ì£¼ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6r1I__Pax5C"
      },
      "outputs": [],
      "source": [
        "# ìƒìœ„ 7ê°œë§Œ ë³´ì—¬ì£¼ë„ë¡ ìˆ˜ì •\n",
        "def pop_category_pie(country):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°œìˆ˜ ì„¸ê¸°\n",
        "    y = country_dfs[country]['category'].value_counts()\n",
        "\n",
        "    # ìƒìœ„ 7ê°œ ì¹´í…Œê³ ë¦¬ì™€ ë‚˜ë¨¸ì§€ í•©ê³„ ê³„ì‚°\n",
        "    top_categories = y[:7]\n",
        "    other_sum = y[7:].sum()\n",
        "\n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    labels = list(top_categories.index) + ['etc']\n",
        "    sizes = list(top_categories.values) + [other_sum]\n",
        "\n",
        "    # ì²« ë²ˆì§¸ ì¡°ê°ì„ ê°•ì¡° í‘œì‹œ (1ìœ„ì¸ ì¹´í…Œê³ ë¦¬)\n",
        "    exp = [0.1] + [0] * (len(labels) - 1)\n",
        "\n",
        "    # ì»¬ëŸ¬ë§µì„ ì‚¬ìš©í•˜ì—¬ ìƒ‰ìƒ ì§€ì •\n",
        "    cmap = plt.get_cmap('Set3')  # 'Set3' ì™¸ì—ë„ ë‹¤ì–‘í•œ ì»¬ëŸ¬ë§µ ì‚¬ìš© ê°€ëŠ¥\n",
        "    colors = cmap(np.linspace(0, 1, len(sizes)))\n",
        "\n",
        "    # íŒŒì´ì°¨íŠ¸ ê·¸ë¦¬ê¸°\n",
        "    plt.pie(sizes, labels=labels, colors = colors, autopct='%.2f%%', startangle=0, explode=exp,  pctdistance=0.85, labeldistance=1.1) #í¼ì„¼íŠ¸ ìˆ«ìê°€ ê²¹ì¹˜ì§€ ì•Šê²Œ distance ì˜µì…˜ì¶”ê°€\n",
        "    plt.title(f'{country}ì˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy8vg17DuAz5"
      },
      "outputs": [],
      "source": [
        "country_detail = ['ë¸Œë¼ì§ˆ','ìºë‚˜ë‹¤','ë…ì¼','í”„ë‘ìŠ¤','ì˜êµ­','ì¸ë„','í•œêµ­','ë©•ì‹œì½”','ëŸ¬ì‹œì•„','ë¯¸êµ­']\n",
        "country = pd.DataFrame(country_list, country_detail, columns = ['ë‚˜ë¼ë³„ ì˜ë¬¸ í‘œê¸°'])\n",
        "display(country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50U21HWgtghM"
      },
      "outputs": [],
      "source": [
        "c = input('ê²€ìƒ‰í•˜ê³ í”ˆ ë‚˜ë¼ì˜ â­ì˜ë¬¸ëª…â­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”! : ')\n",
        "pop_category_pie(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu_plbbY8BOD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}